{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "This is an abstract (basically a reduction of David Silver's Presentation) <br/>\n",
    "http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MDP.pdf\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Processes\n",
    "\n",
    "## Introduction to MDPs\n",
    "- Markov decision processes formally describe an environment for RL\n",
    "- Where the environment is *fully* observable\n",
    "- i.e. the current state completely characterises the process\n",
    "- Almost all RL problems can be formalised as MDPs, e.g.\n",
    "  + Optimal control primarily deals with continuous MDPs\n",
    "  + Partially observable problems can be converted into MDPs\n",
    "  + Bandits are MDPs with one state\n",
    "\n",
    "## Markov Property\n",
    "- \"The future is independent of the past given the present\"\n",
    "- A state $S_t$ is *Markov* if and only if\n",
    "  $$ P[S_{t+1} \\mid S_t] = P[S_{t+1} \\mid S_1, ..., S_t] $$\n",
    "- The state captures all relevant information from the history\n",
    "- Once the state is known, the history may be thrown away\n",
    "- i.e. The state is a sufficient statistic of the future\n",
    "\n",
    "### State Transition Matrix\n",
    "- For a Markov state $s$ and successor state $s'$, the state transition probability is defined by\n",
    "  $$ \\mathcal{P}_{s s'} = P[S_{t+1}=s' \\mid S_t=s] $$\n",
    "- **State transition matrix** $\\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s'$,\n",
    "  $$ \\mathcal{P} =  \\begin{bmatrix} \\mathcal{P}_{1 1} & \\dots  & \\mathcal{P}_{1 n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\mathcal{P}_{n 1} & \\dots  & \\mathcal{P}_{n n} \\end{bmatrix} $$\n",
    "  where each row of the matrix sums to 1\n",
    "  + row: \"from\"\n",
    "  + column: \"to\"\n",
    "\n",
    "## Markov Process\n",
    "- A Markov process is a memoryless random process, i.e. a sequence of random states $S_1, S_2, \\dots$ with the Markov property\n",
    "- A **Markov Process** (or **Markov Chain**) is a tuple $< \\mathcal{S}, \\mathcal{P} >$\n",
    "  + $ \\mathcal{S} $ is a (finite) set of states\n",
    "  + $ \\mathcal{P} $ is a state transition probability matrix,\n",
    "    $$ \\mathcal{P}_{s s'} = P[S_{t+1}=s' \\mid S_t=s] $$\n",
    "- **episode** for a Markov Chain: $ S_1, S_2, ..., S_t $\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Processes\n",
    "- A Markov reward process is a Markov chain with values.\n",
    "- A **Markov Reward Process** is a tuple $< \\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma >$\n",
    "  + $ \\mathcal{S} $ is a (finite) set of states\n",
    "  + $ \\mathcal{P} $ is a state transition probability matrix,\n",
    "    $$ \\mathcal{P}_{s s'} = P[S_{t+1}=s'| S_t=s] $$\n",
    "  + $ \\mathcal{R} $ is a reward function, $ \\mathcal{R}_s = E[R_{t+1} | S_t=s] $\n",
    "  + $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "## Return\n",
    "- The **return** $G_t$ is the total discounted reward from time-step $t$\n",
    "  $$ G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$\n",
    "- The **discount** $\\gamma \\in [0,1]$ is the present value of future rewards\n",
    "- The value of receiving reward $R$ after $k +1$ time-steps is $\\gamma^k R$\n",
    "- This values immediate reward above delayed reward\n",
    "  + $\\gamma$ close to 0 leads to \"myopic\" evaluation\n",
    "  + $\\gamma$ close to 1 leads to \"far-sighted\" evaluation\n",
    "\n",
    "## Why discount?\n",
    "- Most Markov reward and decision processes are discounted. Why?\n",
    "  + Mathematically convenient to discount rewards\n",
    "  + Avoids infinite returns in cyclic Markov processes\n",
    "  + Uncertainty about the future may not be fully represented\n",
    "  + If the reward is financial, immediate rewards may earn more interest than delayed rewards\n",
    "  + Animal/human behaviour shows preference for immediate reward\n",
    "  + It is sometimes possible to use undiscounted Markov reward processes (i.e. $\\gamma=1$), e.g. if all sequences terminate.\n",
    "\n",
    "## Value Function\n",
    "- The value function $v(s)$ gives the long-term value of state $s$\n",
    "- The **state value function** $v(s)$ of a MRP is the expected return starting from state $s$\n",
    "  $$ v(s) = E[G_t | S_t=s] $$\n",
    "- E.g. returns sampled from a MRP: $G_1 = R_2 + \\gamma R_3 + ... + \\gamma^{T-2}R_T$\n",
    "\n",
    "## Bellman Equation for MRPs\n",
    "- The value function can be decomposed into two parts:\n",
    "  + immediate reward $R_{t+1}$\n",
    "  + discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "  $$\\begin{equation} \\begin{aligned} v(s) &= E[G_t | S_t=s] \\\\ &= E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t=s] \\\\ &= E[R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots) \\mid S_t=s] \\\\ &= E[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s] \\\\ &= E[R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t=s] \\\\ &= \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{s s'} v(s') \\end{aligned} \\end{equation}$$\n",
    "  \n",
    "\n",
    "## Bellman Equation in Matrix Form\n",
    "The Bellman equation can be expressed concisely using matrices,\n",
    "$$v = \\mathcal{R} + \\gamma \\mathcal{P} v$$\n",
    "where $v$ is a column vector with one entry per state\n",
    "$$ \\begin{bmatrix} v(1) \\\\ \\vdots \\\\ v(n) \\end{bmatrix} = \\begin{bmatrix} \\mathcal{R}_1 \\\\ \\vdots \\\\ \\mathcal{R}_n \\end{bmatrix} + \\gamma \\begin{bmatrix} \\mathcal{P}_{1 1} & \\dots  & \\mathcal{P}_{1 n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\mathcal{P}_{n 1} & \\dots  & \\mathcal{P}_{n n} \\end{bmatrix} \\begin{bmatrix} v(1) \\\\ \\vdots \\\\ v(n) \\end{bmatrix} $$\n",
    "\n",
    "## Solving the Bellman Equation\n",
    "- The Bellman equation is a linear equation\n",
    "- It can be solved directly:\n",
    "  $$\\begin{equation} \\begin{aligned} v &= \\mathcal{R} + \\gamma \\mathcal{P} v \\\\ (I - \\gamma \\mathcal{P}) v &= \\mathcal{R} \\\\ v &= (I - \\gamma \\mathcal{P})^{-1} \\mathcal{R} \\end{aligned} \\end{equation}$$\n",
    "- Computational complexity is $O(n^3)$ for $n$ states\n",
    "- Direct solution only possible for small MRPs\n",
    "- There are many iterative methods for large MRPs, e.g.\n",
    "  + Dynamic programming\n",
    "  + Monte-Carlo evaluation\n",
    "  + Temporal-Difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "- A Markov decision process (**MDP**) is a Markov reward process with decisions. It is an environment in which all states are Markov.\n",
    "- A **Markov decision process** (**MDP**) is a tuple $< \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma >$\n",
    "  + $\\mathcal{S}$ is a finite set of states\n",
    "  + **$\\mathcal{A}$ is a finite set of actions**\n",
    "  + $\\mathcal{P}$ is a state transition probability matrix,\n",
    "    $$ \\mathcal{P}_{s s'}^\\mathbf{a} = P[S_{t+1}=s' \\mid S_t=s,A_t=\\mathbf{a}] $$\n",
    "  + $\\mathcal{R}$ is a reward function, $ \\mathcal{R}_s^\\mathbf{a} = E[ \\mathcal{R}_{t+1} \\mid S_t=s,A_t=\\mathbf{a} ] $\n",
    "  + $\\gamma$ is a discount factor $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "- A **policy** `$\\pi$` is a distribution over given states,\n",
    "  $$ \\pi(a|s) = P[A_t=a | S_t=s] $$\n",
    "- A policy fully defines the behaviour of an agent\n",
    "- MDP policies depend on the current state (not the history)\n",
    "- i.e. Policies are *stationary* (time-independent),\n",
    "  $$ A_t \\sim \\pi(\\cdot \\mid S_t), \\forall t > 0 $$\n",
    "- Given a MDP $ \\mathcal{M} = < \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma > $ and a policy $\\pi$\n",
    "- The state sequence $ S_1, S_2, ... $ is a Markov process $< \\mathcal{S}, \\mathcal{P}^\\pi >$\n",
    "- The state and reward sequence $ S_1, R_2, S_2, ... $ is a Markov reward process $ <\\mathcal{S}, \\mathcal{P}^\\pi, \\mathcal{R}^\\pi, \\gamma >$\n",
    "- where\n",
    "$$\n",
    "\\mathcal{P}_{s,s'}^\\pi = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{P}_{s s'}^a\n",
    "$$\n",
    "$$\n",
    "\\mathcal{R}_{s}^\\pi = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\mathcal{R}_{s}^a\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "- The **state-value function** $v_\\pi(s)$ of a MDP is the expected return starting from state $s$, and then following policy $\\pi$\n",
    "$$ v_\\pi(s) = E_\\pi[G_t \\mid S_t=s] $$\n",
    "\n",
    "- The **action-value function** $q_\\pi(s,a)$ is the expected return starting from state $s$,  taking action $a$, and then following policy $\\pi$\n",
    "$$ q_\\pi(s,a) = E_\\pi[G_t \\mid S_t=s, A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equation\n",
    "\n",
    "- The state-value function can again be descomposed into immediate reward plus discounted value of successor state,\n",
    "$$ v_\\pi(s) = E_\\pi [R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s] $$\n",
    "\n",
    "- The action-value function can similarly be decomposed,\n",
    "$$ q_\\pi(s,a) = E_\\pi [R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1}) \\mid S_t=s, A_t=a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation for $V^\\pi$\n",
    "<center>\n",
    "<div style=\"width: 60%; height:auto; display:table;\">\n",
    "    <div style=\"width: 60%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/bellman_expectation_eqV1.png\" alt=\"bellman_expectation_eqV1.png\" >\n",
    "        <center> ** D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "$$v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) q_\\pi(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation for $Q^\\pi$\n",
    "<center>\n",
    "<div style=\"width: 60%; height:auto; display:table;\">\n",
    "    <div style=\"width: 60%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/bellman_expectation_eqQ1.png\" alt=\"bellman_expectation_eqQ1.png\" >\n",
    "        <center>** D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "$$q_\\pi(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{s s'}^a v_{\\pi}(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation for $v_\\pi$ (2)\n",
    "<center>\n",
    "<div style=\"width: 60%; height:auto; display:table;\">\n",
    "    <div style=\"width: 60%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/bellman_expectation_eqV2.png\" alt=\"bellman_expectation_eqV2.png\" >\n",
    "        <center>D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "$$v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left(\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{s s'}^a v_{\\pi}(s')\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation for $q_\\pi$ (2)\n",
    "<center>\n",
    "<div style=\"width: 60%; height:auto; display:table;\">\n",
    "    <div style=\"width: 60%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/bellman_expectation_eqQ2.png\" alt=\"bellman_expectation_eqQ2.png\" >\n",
    "        <center>D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "$$q_\\pi(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{s s'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s') q_\\pi(s', a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation (Matrix Form)\n",
    "The Bellman expectation equation can be expressed concisely using the induced MRP,\n",
    "$$ v_{\\pi} = \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_\\pi $$\n",
    "with direct solution\n",
    "$$ v_{\\pi} = (I - \\gamma \\mathcal{P}^\\pi)^{-1}\\mathcal{R}^{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Value Function\n",
    "- The **optimal state-value function** $v_*(s)$ is the maximum value function over all policies\n",
    "$$ v_*(s) = \\max_\\pi v_\\pi(s)$$\n",
    "- The **optimal action-value function** $q_*(s)$ is the maximum action-value function over all policies\n",
    "$$ q_*(s,a) = \\max_\\pi q_\\pi(s,a) $$\n",
    "- The optimal value function specifies the best possible performance in the MDP\n",
    "- A MDP is \"solved\" when we know the optimal value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policy\n",
    "- Define a partial ordering over policies\n",
    "$$ \\pi \\geq \\pi' \\quad \\text{if} \\quad v_\\pi(s) \\geq v_{\\pi'(s),\\quad \\forall s}$$\n",
    "<br/>\n",
    "> **Theorem**: For any Markov Decision Process:\n",
    "  + There exist an optimal policy $\\pi_*$ that is better than or equal to all policies, $\\pi_* \\geq \\pi, \\forall \\pi$\n",
    "  + All optimal policies achieve the optimal value function, $ v_{\\pi_*}(s) = v_*(s) $\n",
    "  + All optimal policies achieve the optimal action-value function, $ q_{\\pi_*}(s,a) = q_*(s,a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an Optimal Policy\n",
    "- An optimal policy can be found by maximising over $q_*(s,a)$,\n",
    "$$ \\pi_*(a \\mid s) = \\begin{cases} \n",
    "      1 & \\text{if}\\: a = \\text{arg} \\max_{a \\in \\mathcal{A}} q_*(s,a) \\\\\n",
    "      0 & otherwise\n",
    "   \\end{cases} $$\n",
    "- There is always a deterministic optimal policy for any MDP\n",
    "- If we know $q_*(s,a)$, we immediately have the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Optimality Equation for $V_*$\n",
    "<center>\n",
    "<div style=\"width: 60%; height:auto; display:table;\">\n",
    "    <div style=\"width: 60%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/bellman_optimality_eqV1.png\" alt=\"bellman_optimality_eqV1.png\" >\n",
    "        <center> ** D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "$$ v_*(s) = \\max_a q_*(s,a) $$\n",
    "- Now the action with the maximum Q-value is taken (not the average as before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Optimality Equation for $Q_*$\n",
    "<center>\n",
    "<div style=\"width: 60%; height:auto; display:table;\">\n",
    "    <div style=\"width: 60%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/bellman_optimality_eqQ1.png\" alt=\"bellman_optimality_eqQ1.png\" >\n",
    "        <center> ** D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "$$ q_*(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{s s'}^a v_*(s') $$\n",
    "- We average all the possible states (not the maximum as before) because we cannot control the transition, the environment does it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Optimality Equation for $V_*$ (2)\n",
    "<center>\n",
    "<div style=\"width: 60%; height:auto; display:table;\">\n",
    "    <div style=\"width: 60%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/bellman_optimality_eqV2.png\" alt=\"bellman_optimality_eqV2.png\" >\n",
    "        <center> ** D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "$$ v_*(s) = \\max_a \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{s s'}^a v_*(s') \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Optimality Equation for $Q_*$ (2)\n",
    "<center>\n",
    "<div style=\"width: 60%; height:auto; display:table;\">\n",
    "    <div style=\"width: 60%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/bellman_optimality_eqQ2.png\" alt=\"bellman_optimality_eqQ2.png\" >\n",
    "        <center> ** D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "$$ q_*(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{s s'}^a \\max_{a'} q_*(s',a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Bellman Optimality Equation\n",
    "- Bellman Optimality Equation is non-linear\n",
    "- No closed form solution (in general)\n",
    "- Many iterative solution methods\n",
    "  + Value Iteration (Dynamic Programming)\n",
    "  + Policy Iteration (Dynamic Programming)\n",
    "  + Q-learning\n",
    "  + SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite MDPs\n",
    "- The following extensions are all possible:\n",
    "  + Countably infinite state and/or action spaces\n",
    "    + Straightforward\n",
    "  + Continuous state and/or action spaces\n",
    "    + Closed form for linear quadratic model (LQR)\n",
    "  + Continuous time\n",
    "    + Requires partial differential equations\n",
    "    + Hamilton-Jacobi-Bellman (HJB) equation\n",
    "    + Limiting case of Bellman equation as time-step $\\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMDPs\n",
    "- A **Partially Observable Markov Decision Process** is a MDP with hidden states. It is a hidden Markov model with actions.\n",
    "- A **POMDP** is a tuple $<\\mathcal{S}, \\mathcal{A}, \\mathcal{O}, \\mathcal{P}, \\mathcal{R}, \\mathcal{Z}, \\gamma>$\n",
    "  + $\\mathcal{S}$ is a finite set of states\n",
    "  + $\\mathcal{A}$ is a finite set of actions\n",
    "  + **$\\mathcal{O}$ is a finite set of observations**\n",
    "  + $\\mathcal{P}$ is a state transtion probability matrix,\n",
    "  $$ \\mathcal{P}_{s s'}^a = P[S_{t+1}=s' \\mid S_t=s,A_t=a] $$\n",
    "  + $\\mathcal{R}$ is a reward function, $ \\mathcal{R}_s^a = E[ \\mathcal{R}_{t+1} \\mid S_t=s,A_t=a ] $\n",
    "  + **$\\mathcal{Z}$ is an observation function**,\n",
    "  $$ \\mathcal{Z}_{s' o}^a = P[O_{t+1}=o \\mid S_{t+1}=s', A_t=\\mathbf{a}] $$\n",
    "  + $\\gamma$ is a discount factor $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belief States\n",
    "- A **history** $H_t$ is a sequence of actions, observations and rewards,\n",
    "$$ H_t = A_0, O_1, R_1, \\dots, A_{t-1}, O_t, R_t $$\n",
    "\n",
    "- A **belief state** $b(h)$ is a probability distribution over states, conditioned on the history $h$\n",
    "  $$ b(h) = (P[S_t=s^1 \\mid H = h],\\dots, P[S_t=s^n \\mid H = h]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction of POMDPs\n",
    "- The history $H_t$ satisfies the Markov property\n",
    "- The belief state $b(H_t)$ satisfies the Markov property\n",
    "\n",
    "<center>\n",
    "<div style=\"width: 80%; height:auto; display:table;\">\n",
    "    <div style=\"width: 80%; hdisplay:table-cell;vertical-align:middle;\">\n",
    "        <img src=\"Figures/history_and_belief_tree.png\" alt=\"history_and_belief_tree.png\" >\n",
    "        <center> ** D.Silver's Figure</center>\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "- A POMDP can be reduced to an (infinite) history tree\n",
    "- A POMDP can be reduced to an (infinite) belief state tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Reward MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ergodic Markov Process\n",
    "An ergodic Markov process is:\n",
    "- **Recurrent**: each state is visited an infinite number of times\n",
    "- **Aperiodic**: each state is visited without any systematic period\n",
    "\n",
    "> **Theorem**: An ergodic Markov process has a limiting stationary distribution $d^\\pi(s)$ with the property\n",
    "> $$ d^\\pi(s) = \\sum_{s' \\in \\mathcal{S}} d^{\\pi}(s') \\mathcal{P}_{s s'} $$\n",
    "\n",
    "- A MDP is **ergodic** if the Markov chain induced by any policy is ergodic\n",
    "- For any policy $\\pi$, an ergodic MDP has an *average reward per time-step* $\\rho^\\pi$ that is independent of start state,\n",
    "$$ \\rho^\\pi = \\lim_{T \\rightarrow \\infty} \\frac{1}{T} E \\left[ \\sum_{t=1}^T R_t \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Reward Value Function\n",
    "- The value function of an undiscounted, ergodic MDP can be expressed in terms of average reward\n",
    "$$ \\widetilde{v}_\\pi = E_\\pi \\left[ \\sum_{k=1}^\\infty (R_{t+k} - \\rho^\\pi) \\mid S_t=s \\right] $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
